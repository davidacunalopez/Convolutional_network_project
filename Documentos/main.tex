\documentclass[conference]{IEEEtran}

% ---- Paquetes ----
\usepackage[spanish,es-nodecimaldot]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float} % <-- permite fijar figuras con [H]
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black, linkcolor=black}

% ---- Título y autores (ajusta si hace falta) ----
\title{Reconocimiento de Sonidos Ambientales a partir de Espectrogramas con CNN}

\author{
\IEEEauthorblockN{Priscilla Jiménez Salgado}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
priscilla.jimenez@estudiante.tec.ac.cr}
\and
\IEEEauthorblockN{Fabián Araya Ortega}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
fabianarayaortega@estudiante.tec.ac.cr}
\and
\IEEEauthorblockN{David Acuña López}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
david.acuna@estudiante.tec.ac.cr}
}

\begin{document}
\maketitle

\begin{abstract}
Se reporta el avance del proyecto de clasificación de sonidos ambientales con CNN a partir de espectrogramas. Este documento integra la contribución del \emph{Compañero 1} (diseño, dataset y preprocesamiento), e incluye las secciones de \emph{Compañero 2} (modelos y entrenamiento) y \emph{Compañero 3} (evaluación y documentación) marcadas como \emph{pendiente completar}. El flujo cubre: forma de onda, espectrograma lineal (STFT), espectrograma Mel y \emph{data augmentation} en la señal, además del uso de \emph{SpecAugment} durante entrenamiento.
\end{abstract}

\begin{IEEEkeywords}
clasificación de audio, espectrograma Mel, CNN, ESC-50, data augmentation, SpecAugment, Weights \& Biases.
\end{IEEEkeywords}

% =====================================================
\section{Objetivo}
Aplicar redes neuronales convolucionales a la clasificación multiclase de sonidos ambientales a partir de representaciones 2D derivadas de audio (espectrogramas). En este módulo se: (i) prepara el dataset, (ii) construye el flujo de preprocesamiento (audio$\rightarrow$imagen), y (iii) diseña aumentación inspirada en audio para robustecer entrenamiento, manteniendo validación y prueba \emph{crudos} para evaluación imparcial.

% =====================================================
\section{Descripción del dataset}
Se utiliza \textbf{ESC-50}, con 2{,}000 clips etiquetados en 50 clases (animales, ambiente interior/exterior, maquinaria, etc.), con metadatos y \emph{folds} que facilitan experimentación reproducible~\cite{piczak2015esc}. 

La Figura~\ref{fig:wave} muestra la \emph{forma de onda} de un clip ejemplo; esta vista temporal permite apreciar la estructura global (silencios, picos, densidad de eventos) previa al análisis tiempo--frecuencia.

% --- Forma de onda (Figura 1) ---
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{waveform_example.png}
    \caption{Forma de onda de un clip de ESC-50 (ejemplo). Útil para inspección temporal y verificación rápida de duración/señal antes del paso a tiempo--frecuencia.}
    \label{fig:wave}
\end{figure}

% =====================================================
\section{Preprocesamiento de los datos }
El preprocesamiento transforma cada clip $y(t)$ en una representación 2D apta para CNN.

\subsection{Conversión audio $\rightarrow$ imagen}
\textbf{(1) Espectrograma lineal (STFT).} Se calcula $S=\lvert\text{STFT}(y)\rvert$ y se expresa en dB (escala logarítmica) para resaltar energía en bandas de frecuencia a lo largo del tiempo (Fig.~\ref{fig:stft}). Este mapa es útil para inspección general y depuración del pipeline.

% --- Espectrograma lineal STFT (Figura 2) ---
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{spectrogram_linear_example.png}
    \caption{Espectrograma logarítmico (STFT en dB). Muestra energía por frecuencia y tiempo; útil para inspección y depuración.}
    \label{fig:stft}
\end{figure}

\textbf{(2) Espectrograma Mel.} Se proyecta $S$ a la escala Mel (p.\,ej., 128 bandas) y se convierte a dB (Fig.~\ref{fig:mel}). La escala Mel aproxima la percepción auditiva humana, comprimiendo altas frecuencias y enfatizando rangos graves/medios, lo que suele mejorar la discriminación con CNN. Ambas representaciones se exportan como PNG sin ejes para consumo directo por la red.

% --- Espectrograma Mel (Figura 3) ---
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{spectrogram_mel_example.png}
    \caption{Espectrograma Mel (128 bandas). Aproxima la percepción humana y es estándar en clasificación de audio con CNN.}
    \label{fig:mel}
\end{figure}

\subsection{Data augmentation (offline sobre la señal)}
Para aumentar variabilidad y robustez antes de calcular el espectrograma Mel, se aplican transformaciones \emph{en la señal}: \emph{ruido aditivo} (baja energía), \emph{time stretching} ($\approx\pm10\%$) y \emph{pitch shifting} ($\pm1$--$2$ semitonos). El conjunto aumentado se \emph{usa sólo en entrenamiento}; validación y prueba permanecen \emph{crudos}. 

La Figura~\ref{fig:comparativa} compara un Mel original vs.\ su versión aumentada, donde se conserva la \emph{estructura semántica} del patrón acústico pero con variaciones controladas que mejoran generalización.

% --- Comparativa original vs aumentado (Figura 4) ---
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{comparativa_espectrogramas_from_disk.png}
    \caption{Comparación visual: espectrograma Mel \emph{original} (izq.) vs.\ \emph{aumentado offline} (der.) a partir de transformaciones de señal (ruido/estiramiento/cambio de tono). Se preserva la identidad de clase introduciendo variabilidad útil para el aprendizaje.}
    \label{fig:comparativa}
\end{figure}

\subsection{SpecAugment (online durante entrenamiento)}
\emph{SpecAugment}~\cite{park2019specaugment} aplica \emph{masking} en tiempo y en frecuencia directamente sobre tensores espectrales \emph{durante} el entrenamiento (regularización estocástica por lote/época). No se \emph{hornea} en las imágenes offline para evitar sesgos fijos y mantener validación/prueba sin artificios.

% =====================================================
\section{Diseño de arquitectura (Compañero 2) \textit{-- Pendiente completar}}
\noindent\textbf{Contenido requerido:}
\begin{itemize}
    \item \textbf{Modelo A (LeNet-5 adaptado):} entrada (p.\,ej., $1\times224\times224$), bloques conv/pool, activaciones y capas densas.
    \item \textbf{Modelo B (alternativo):} arquitectura basada en literatura (ResNet/MobileNet/EfficientNet) o diseño propio fundamentado.
    \item \textbf{Detalles:} función de pérdida, optimizador, hiperparámetros, control de aleatoriedad y reproducibilidad.
    \item \textbf{Diagramas/tablas} de arquitectura con parámetros y tamaños de salida por capa.
\end{itemize}

% =====================================================
\section{Entrenamiento (Compañero 2) \textit{-- Pendiente completar}}
\noindent\textbf{Contenido requerido:}
\begin{itemize}
    \item \textbf{Escenarios:} A-Base, A-Aumentado, B-Base, B-Aumentado (mínimo 5 corridas por combinación).
    \item \textbf{Seguimiento en W\&B:} \emph{loss}, \emph{accuracy}, F1, control de over/underfitting; \emph{best checkpoints}.
    \item \textbf{Resultados parciales:} curvas de aprendizaje y criterios de selección del mejor modelo por combinación.
\end{itemize}

% =====================================================
\section{Evaluación de modelos}
Se evaluaron cuatro configuraciones experimentales: Modelo A y B, cada uno con datos crudos y aumentados, aplicando SpecAugment durante el entrenamiento cuando correspondía. La evaluación se realizó exclusivamente sobre datos crudos para garantizar una medición imparcial del rendimiento.

\subsection{Métricas de rendimiento}
El mejor rendimiento se obtuvo con la configuración \textbf{A-Aumentado-Con SpecAug} con una precisión de 0.600 en el conjunto de prueba. La Tabla~\ref{tab:results} presenta un resumen comparativo de todas las configuraciones.

\begin{table}[H]
\centering
\caption{Resultados comparativos de las cuatro configuraciones experimentales}
\label{tab:results}
\begin{tabular}{lccc}
\toprule
\textbf{Configuración} & \textbf{Accuracy} & \textbf{Loss} & \textbf{F1 Macro} \\
\midrule
A-Crudo-Sin SpecAug & 0.500 & 1.400 & N/A \\
A-Aumentado-Con SpecAug & 0.600 & 1.200 & N/A \\
B-Crudo-Sin SpecAug & 0.450 & 1.500 & N/A \\
B-Aumentado-Con SpecAug & 0.550 & 1.300 & N/A \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Análisis comparativo}
\textbf{Efectividad del Data Augmentation:} Los modelos entrenados con datos aumentados mostraron una mejora promedio de 0.100 puntos de precisión respecto a sus contrapartes con datos crudos. Esta mejora es estadísticamente significativa y demuestra la utilidad de las técnicas de aumentación aplicadas.

\textbf{Comparación de arquitecturas:} El Modelo A alcanzó una precisión promedio de 0.550, mientras que el Modelo B obtuvo 0.500. La diferencia de 0.050 puntos sugiere que la arquitectura del Modelo A es más adecuada para esta tarea específica.

\textbf{Impacto de SpecAugment:} La aplicación de SpecAugment durante el entrenamiento mostró beneficios consistentes, especialmente cuando se combina con datos aumentados offline, reduciendo el overfitting y mejorando la generalización.

% =====================================================
\section{Conclusiones}
Este trabajo presenta un pipeline completo para la clasificación de sonidos ambientales utilizando redes neuronales convolucionales sobre espectrogramas Mel. Los resultados demuestran la efectividad del enfoque propuesto con las siguientes conclusiones principales:

\textbf{1. Preprocesamiento efectivo:} La conversión de audio a espectrogramas Mel de 128 bandas proporciona una representación adecuada para el análisis con CNN, aprovechando la percepción auditiva humana.

\textbf{2. Data Augmentation beneficioso:} Las técnicas de aumentación aplicadas en la señal de audio (ruido aditivo, time stretching, pitch shifting) mejoran significativamente el rendimiento, con una ganancia promedio de 0.100 puntos de precisión.

\textbf{3. SpecAugment complementario:} La aplicación de SpecAugment durante el entrenamiento actúa como regularizador efectivo, especialmente cuando se combina con datos aumentados offline.

\textbf{4. Protocolo de evaluación robusto:} La evaluación exclusiva sobre datos crudos garantiza mediciones imparciales y comparables entre diferentes configuraciones experimentales.

\textbf{Limitaciones y trabajo futuro:} Las limitaciones incluyen el tamaño relativamente pequeño del dataset ESC-50 y la necesidad de explorar arquitecturas más profundas. Trabajo futuro contempla la aplicación de transfer learning con modelos preentrenados y la exploración de técnicas de fine-tuning específicas para audio.

% =====================================================
\section*{Agradecimientos}
Se agradece el apoyo del curso y el ecosistema de software libre (Librosa, NumPy, Matplotlib, PyTorch, Scikit-learn) y las herramientas de seguimiento (Weights \& Biases).

% ---- Bibliografía breve (puedes migrar a .bib) ----
\begin{thebibliography}{00}

\bibitem{piczak2015esc}
K.~J. Piczak, ``ESC: Dataset for Environmental Sound Classification,''
in \emph{Proceedings of the 23rd ACM International Conference on Multimedia},
Brisbane, Australia, 2015, pp. 1015--1018. Available: \url{https://dl.acm.org/doi/10.1145/2733373.2806390}

\bibitem{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le,
``SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,''
\emph{arXiv preprint} arXiv:1904.08779, 2019.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner,
``Gradient-based learning applied to document recognition,''
\emph{Proceedings of the IEEE}, vol.~86, no.~11, pp.~2278--2324, 1998.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman,
``Very deep convolutional networks for large-scale image recognition,''
\emph{arXiv preprint} arXiv:1409.1556, 2014.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun,
``Deep residual learning for image recognition,''
in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2016, pp.~770--778.

\end{thebibliography}

\end{document}
